{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Project of Reinforcement Learning: Stupid Vautour\n\nAuthors: Clément Côme, Valentin Gatignol, Nathan De Carvalho",
      "metadata": {
        "tags": [],
        "cell_id": "00001-88027250-8086-4591-8e2b-650b9f660dbd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-f459d0c7-fd86-4895-ba29-3d0c7fcb322a",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c395d4ed",
        "execution_millis": 101,
        "execution_start": 1618650597997,
        "deepnote_cell_type": "code"
      },
      "source": "import gym\nimport numpy as np\nimport random\nfrom gym import spaces\nfrom gym.utils import seeding\nimport math\nfrom scipy.special import comb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 1) Objectives of the project",
      "metadata": {
        "tags": [],
        "cell_id": "00002-ae4d5ecf-7fc4-4f52-a0d2-035d51607e51",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.1) Rules of the game",
      "metadata": {
        "tags": [],
        "cell_id": "00002-7977bc56-59e7-4c45-9b87-a63877ba42b8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Stupide Vautour is a turn by turn game.\n\n1. Every player is given 15 cards from 1 to 15. A player can see only its cards and can play each of them only once.\n2. A deck of 5 malus cards (-5 to -1) and 10 bonus cards (1 to 10) is shuffled.\n3. For each card in the deck:\n   1. Turn over the bonus/malus card\n   2. Each player chooses a card from its hand that he want to play during this turn and put it face down so no one sees it.\n   3. When every player has put down its card, they can turn it face up simultaneously.\n   4. - If the card is a malus the player that played the smallest card gets it.\n      - If the card is a bonus the player that played the highest card gets it.\n      - In case of a tie, the card goes to the next smallest (or highest) card.\n      - If all players paly the same card, it is discarded and not one gets the card.\n4. The final score of each player is computed with the sum of bonuses minus the sum of maluses.\n5. The winner is the player with the highest final score\n\nThe real rules have other slight details that are not considered here.",
      "metadata": {
        "tags": [],
        "cell_id": "00002-c76d9d1a-c327-4d75-ada3-93ce0f78bbb8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.2) Implementing the game",
      "metadata": {
        "tags": [],
        "cell_id": "00004-1a559f3a-422f-440e-a3a6-be7e84d3eb94",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Implementation of \"Stupide Vautour\" and artificial players.\n\nThe project is divided in several parts:\n\n- `implementation/game.py` defines the game rules and flow, an example of its use is given at the end of the file\n- `implementation/player.py` defines the players that can be used to play the game\n- `implementation/analyse.py` contains several functions to process the results of the games played",
      "metadata": {
        "tags": [],
        "cell_id": "00005-851d3c37-cb42-4071-bfec-f0a6d0363347",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.3) Apply Reinforcement Learning Principles to \"win the game\"",
      "metadata": {
        "tags": [],
        "cell_id": "00006-218d7bd2-e21d-4c0c-8c7d-f270d9a8c9cc",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "***Intuitive preliminary comments:***\n\nWhen discussing intuitively what factors should help a \"learning\" player to find an optimal strategy to beat his adversaries, we point out several issues including:\n - The number of players and the definition of the associated state space\n - The nature of the adversaries: what kind of strategies are they playing?\n - The learning process being *on-line* (i.e. the player learns while playing) or *off-line* (i.e. the player learns by observing people playing and then can play and use his off-line trained strategy)\n\n\n***A priori objectives of the implementation:***\n\n- Study the game thoroughly for the cases with 2 and 3 players: State space, a priori good and bad strategies.\n\n- Implement several types of artificial players and comment on the game's results.\n\n- Given 1 (or 2) adversaries with fixed strategies, implemented RL algorithms to compute the value function (when the state space is not too large) or to estimate it (when the state space is very large).\n\n- (If enough time) Given a set of strategies, 1 (or 2) adversaries with fixed strategies, choose the strategy that makes us beat the adversaries (apply bandits-based RL principles).",
      "metadata": {
        "tags": [],
        "cell_id": "00005-da4ad5f7-cb5f-41f9-8c35-edc231d1472c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 2) Definition of the State Space and some ad hoc strategies",
      "metadata": {
        "tags": [],
        "cell_id": "00008-ae76a98f-53db-4cdc-b4c3-c17c18b738d6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 2.1) Possible actions and State space definitions",
      "metadata": {
        "tags": [],
        "cell_id": "00009-8945194b-16fc-47d1-b443-cfa871386dc7",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n#### A reducing set of actions\n\nThe number of actions each player can take decreases by one at each turn as he uses one card so that the aim is to find the best moments when to play a card given the state of the game.\n\nFor a given number of players $N$, we will define two types of State Spaces (SS) from which the player will have to play a card which remains in his hand - both being exposed to the curse of dimentionality:\n\n#### No-Memory State Space (NMSS): *easy case*\n\nAssume a naive no-memory player only takes into account:\n\n- The current deck's card\n- His remaining cards\n\nSuch a NMSS **does not depend on the number of players** and this approach will allow to scale easily RL methods to games with numerous players.\n\nYet, such NMSS is already very large:\n\n$$\n|NMSS| = \\text{Number of deck's cards}*\\sum_{\\text{Nber remaining cards} = 1}^{\\text{Nber of possible cards}} \\binom{\\text{Nber of possible cards}}{\\text{Nber remaining cards}}\n$$\n\nWhich is $491505$ possible states for the orignal game with $15$ cards.\n\n*Remark:* we could simplify the game to get reasonable size for the state pace: e.g. for N_cards = 6, we get 378 states so that we can avoid approximating the value function for that case.\n\n\n#### Machine State Space (MSS): *difficult case*\n\nWe consider all the information available to the player, which includes:\n- The current deck's card\n- The remaining deck's cards\n\n- His remaining cards\n- His current score\n- The other players' current scores\n- The other players' remaining cards\n\nClearly, **the MSS depends on the number of players and its dimensionality explodes when considering all these information** - and we will have no choice but approximating the value function to deal with such SS.",
      "metadata": {
        "tags": [],
        "cell_id": "00010-6b12b0c5-edfb-4a10-b83a-c2b91eb4bd56",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00011-58ad4369-441d-4909-85a9-13cdcafc12e6",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b93c25ee",
        "execution_millis": 5,
        "execution_start": 1618653666215,
        "deepnote_cell_type": "code"
      },
      "source": "# Number of states in NMSS\n\n#V1\n\nNber_states = 6*(2**6-1)\n\n#V2\n\nN_cards = 6\n\ncombinaisons = np.zeros(N_cards)\nfor i in range(N_cards):\n    combinaisons[i] = comb(N_cards,i+1)\nNber_states = N_cards*np.sum(combinaisons) \n\nprint('The number of states for the NMSS is: ', Nber_states, ' with N_cards = ', N_cards)\n",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "The number of states for the NMSS is:  378.0  with N_cards =  6\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-7c2cc72e-95bc-47f3-b59d-712101e3fd23",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### 2.2) Some proposed strategies",
      "metadata": {
        "tags": [],
        "cell_id": "00015-6952b0dc-6558-459c-93ac-b708b8ab539d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Very basic strategies - action independent of the deck's card**\n\n- *Random player:* plays a card randomly with a distribution independent of the game's information\n- *Max card player:* always plays his maximum card whatever the deck's card\n- *Min card player:* always plays his min card whatever the deck's card\n\n**Intermediate strategies - oberve only the deck's card to decide action**\n\n- *Bonus craving player:* plays with high probability a high card to gain high bonuses, and plays randomly for the rest\n- *Malus adverse player:* plays with high probability high cards for maluses, and plays randomly for the rest\n- *GetRidOfBadCards player:* plays with high probability low cards on small maluses or bonuses and randomly for the rest\n\n",
      "metadata": {
        "tags": [],
        "cell_id": "00016-bc48fd93-e09d-4a11-b7a6-9ad2d3654cab",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00017-156d8a29-495a-4666-b35c-205135bbd4ca",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "adbaed0f",
        "execution_millis": 2,
        "execution_start": 1618478991419,
        "deepnote_cell_type": "code"
      },
      "source": "from game import *",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00017-b601d490-7a49-4b16-896d-cbc017ada1e2",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c3570100-157e-4a3f-b5c9-ebc0870008b3' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "9fa24aad-cf1b-466a-8713-93cab9ec6449",
    "deepnote_execution_queue": []
  }
}