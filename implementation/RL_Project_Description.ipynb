{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Project of Reinforcement Learning: Stupid Vautour\n\nAuthors: Clément Côme, Valentin Gatignol, Nathan De Carvalho",
      "metadata": {
        "tags": [],
        "cell_id": "00001-88027250-8086-4591-8e2b-650b9f660dbd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-dfd962a1-0e8c-45a1-8e3f-376bb30ac2f6",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a6775928",
        "execution_millis": 6980,
        "execution_start": 1618143413276,
        "deepnote_cell_type": "code"
      },
      "source": "!pip install gym==0.18.0",
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting gym==0.18.0\n  Downloading gym-0.18.0.tar.gz (1.6 MB)\n\u001b[K     |████████████████████████████████| 1.6 MB 16.5 MB/s \n\u001b[?25hRequirement already satisfied: scipy in /shared-libs/python3.7/py/lib/python3.7/site-packages (from gym==0.18.0) (1.6.1)\nRequirement already satisfied: numpy>=1.10.4 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from gym==0.18.0) (1.19.5)\nCollecting pyglet<=1.5.0,>=1.4.0\n  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n\u001b[K     |████████████████████████████████| 1.0 MB 55.9 MB/s \n\u001b[?25hCollecting Pillow<=7.2.0\n  Downloading Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n\u001b[K     |████████████████████████████████| 2.2 MB 62.2 MB/s \n\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0\n  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\nRequirement already satisfied: future in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.18.0) (0.18.2)\nBuilding wheels for collected packages: gym\n  Building wheel for gym (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.18.0-py3-none-any.whl size=1656450 sha256=238a6ea7630a1d07b6b73f5bda4290f0442668db340d0fa75664d0f021a77967\n  Stored in directory: /root/.cache/pip/wheels/99/f7/e3/d6f0f120ac047c1e5de2ae34930e7bf6e8de1c7a4d5fa68555\nSuccessfully built gym\nInstalling collected packages: pyglet, Pillow, cloudpickle, gym\n  Attempting uninstall: Pillow\n    Found existing installation: Pillow 8.1.2\n    Not uninstalling pillow at /shared-libs/python3.7/py/lib/python3.7/site-packages, outside environment /root/venv\n    Can't uninstall 'Pillow'. No files were found to uninstall.\nSuccessfully installed Pillow-7.2.0 cloudpickle-1.6.0 gym-0.18.0 pyglet-1.5.0\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-f459d0c7-fd86-4895-ba29-3d0c7fcb322a",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c395d4ed",
        "execution_millis": 84,
        "execution_start": 1618143420302,
        "deepnote_cell_type": "code"
      },
      "source": "import gym\nimport numpy as np\nimport random\nfrom gym import spaces\nfrom gym.utils import seeding\nimport math\nfrom scipy.special import comb",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 1) Objectives of the project",
      "metadata": {
        "tags": [],
        "cell_id": "00002-ae4d5ecf-7fc4-4f52-a0d2-035d51607e51",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.1) Rules of the game",
      "metadata": {
        "tags": [],
        "cell_id": "00002-7977bc56-59e7-4c45-9b87-a63877ba42b8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Stupide Vautour is a turn by turn game.\n\n1. Every player is given 15 cards from 1 to 15. A player can see only its cards and can play each of them only once.\n2. A deck of 5 malus cards (-5 to -1) and 10 bonus cards (1 to 10) is shuffled.\n3. For each card in the deck:\n   1. Turn over the bonus/malus card\n   2. Each player chooses a card from its hand that he want to play during this turn and put it face down so no one sees it.\n   3. When every player has put down its card, they can turn it face up simultaneously.\n   4. - If the card is a malus the player that played the smallest card gets it.\n      - If the card is a bonus the player that played the highest card gets it.\n      - In case of a tie, the card goes to the next smallest (or highest) card.\n      - If all players paly the same card, it is discarded and not one gets the card.\n4. The final score of each player is computed with the sum of bonuses minus the sum of maluses.\n5. The winner is the player with the highest final score\n\nThe real rules have other slight details that are not considered here.",
      "metadata": {
        "tags": [],
        "cell_id": "00002-c76d9d1a-c327-4d75-ada3-93ce0f78bbb8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.2) Implementing the game",
      "metadata": {
        "tags": [],
        "cell_id": "00004-1a559f3a-422f-440e-a3a6-be7e84d3eb94",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Implementation of \"Stupide Vautour\" and artificial players.\n\nThe project is divided in several parts:\n\n- `implementation/game.py` defines the game rules and flow, an example of its use is given at the end of the file\n- `implementation/player.py` defines the players that can be used to play the game\n- `implementation/analyse.py` contains several functions to process the results of the games played",
      "metadata": {
        "tags": [],
        "cell_id": "00005-851d3c37-cb42-4071-bfec-f0a6d0363347",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.3) Apply Reinforcement Learning Principles to \"win the game\"",
      "metadata": {
        "tags": [],
        "cell_id": "00006-218d7bd2-e21d-4c0c-8c7d-f270d9a8c9cc",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "***Intuitive preliminary comments:***\n\nWhen discussing intuitively what factors should help a \"learning\" player to find an optimal strategy to beat his adversaries, we point out several issues including:\n - The number of players and the definition of the associated state space\n - The nature of the adversaries: what kind of strategies are they playing?\n - The learning process being *on-line* (i.e. the player learns while playing) or *off-line* (i.e. the player learns by observing people playing and then can play and use his off-line trained strategy)\n\n\n***A priori objectives of the implementation:***\n\n- Study the game thoroughly for the cases with 2 and 3 players: State space, a priori good and bad strategies.\n\n- Implement several types of artificial players and comment on the game's results.\n\n- Given 1 (or 2) adversaries with fixed strategies, implemented RL algorithms to compute the value function (when the state space is not too large) or to estimate it (when the state space is very large).\n\n- (If enough time) Given a set of strategies, 1 (or 2) adversaries with fixed strategies, choose the strategy that makes us beat the adversaries (apply bandits-based RL principles).",
      "metadata": {
        "tags": [],
        "cell_id": "00005-da4ad5f7-cb5f-41f9-8c35-edc231d1472c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 2) Definition of the State Space and some ad hoc strategies",
      "metadata": {
        "tags": [],
        "cell_id": "00008-ae76a98f-53db-4cdc-b4c3-c17c18b738d6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 2.1) Possible actions and State space definitions",
      "metadata": {
        "tags": [],
        "cell_id": "00009-8945194b-16fc-47d1-b443-cfa871386dc7",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n#### A reducing set of actions\n\nThe number of actions each player can take decreases by one at each turn as he uses one card so that the aim is to find the best moments when to play a card given the state of the game.\n\nFor a given number of players $N$, we will define two types of State Spaces (SS) from which the player will have to play a card which remains in his hand - both being exposed to the curse of dimentionality:\n\n#### No-Memory State Space (NMSS): *easy case*\n\nAssume a naive no-memory player only takes into account:\n\n- The current deck's card\n- His remaining cards\n\nSuch a NMSS **does not depend on the number of players** and this approach will allow to scale easily RL methods to games with numerous players.\n\nYet, such NMSS is already very large:\n\n$$\n|NMSS| = \\text{Number of deck's cards}*\\sum_{\\text{Nber remaining cards} = 1}^{\\text{Nber of possible cards}} \\binom{\\text{Nber of possible cards}}{\\text{Nber remaining cards}}\n$$\n\nWhich is $491505$ possible states for the orignal game with $15$ cards.\n\n*Remark:* we could simplify the game to get reasonable size for the state pace: e.g. for N_cards = 6, we get 378 states so that we can avoid approximating the value function for that case.\n\n\n#### Machine State Space(MSS): *difficult case*\n\nWe consider all the information available to the player, which includes:\n- The current deck's card\n- The remaining deck's cards\n\n- His remaining cards\n- His current score\n- The other players' current scores\n- The other players' remaining cards\n\nClearly, **the MSS depends on the number of players and its dimensionality explodes when considering all these information** - and we will have no choice but approximating the value function to deal with such SS.",
      "metadata": {
        "tags": [],
        "cell_id": "00010-6b12b0c5-edfb-4a10-b83a-c2b91eb4bd56",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00011-58ad4369-441d-4909-85a9-13cdcafc12e6",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "fab52460",
        "execution_millis": 6,
        "execution_start": 1618143689988,
        "deepnote_cell_type": "code"
      },
      "source": "# Number of states in NMSS\n\n#V1\n\nNber_states = 15*2**15\n\n#V2\n\nN_cards = 15\n\ncombinaisons = np.zeros(N_cards)\nfor i in range(N_cards):\n    combinaisons[i] = comb(N_cards,i+1)\nNber_states = N_cards*np.sum(combinaisons) \n\nprint('The number of states for the NMSS is: ', Nber_states, ' with N_cards = ', N_cards)",
      "outputs": [
        {
          "name": "stdout",
          "text": "The number of states for the NMSS is:  491505.0  with N_cards =  15\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### 2.2) Some proposed strategies",
      "metadata": {
        "tags": [],
        "cell_id": "00015-6952b0dc-6558-459c-93ac-b708b8ab539d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Very basic strategies - action independent of the deck's card**\n\n- *Random player:* plays a card randomly with a distribution independent of the game's information\n- *Max card player:* always plays his maximum card whatever the deck's card\n- *Min card player:* always plays his min card whatever the deck's card\n\n**Intermediate strategies - oberve only the deck's card to decide action**\n\n- *Bonus craving player:* plays with high probability a high card to gain high bonuses, and plays randomly for the rest\n- *Negative adverse player:* plays high cards for maluses and low cards for bonuses\n- *GetRidOfBadCards player:* plays with high probability low cards on small maluses or bonuses\n\n",
      "metadata": {
        "tags": [],
        "cell_id": "00016-bc48fd93-e09d-4a11-b7a6-9ad2d3654cab",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c3570100-157e-4a3f-b5c9-ebc0870008b3' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "9fa24aad-cf1b-466a-8713-93cab9ec6449",
    "deepnote_execution_queue": []
  }
}